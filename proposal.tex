\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
% \usepackage[numbers]{natbib}
% \usepackage{authordate1-4}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{todonotes}
\usepackage{newclude}
\usepackage{float}

% \def\op#1{\textcolor{red}{TODO\@ \textit{#1}}}

\def\op#1{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red]{#1}}

%% Compile with:
%% latexmk -pdf -interaction=nonstopmode -synctex=1 -pvc proposal.tex

%%%%%% PLEASE USE THIS DECLARATION FOR YOUR COMMENTS %%%%%%%%

% OD: this is needed for strike through
\usepackage[normalem]{ulem}

% OP - Ondrej Platek inline 
\def\OP#1{{\color{purple}OP: \it #1}}
\def\ODdel#1{\bgroup\markoverwith{\textcolor{purple}{\rule[0.5ex]{2pt}{1pt}}}\ULon{#1}}

% OD - Ondrej Dusek
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\def\OD#1{{\color{darkgreen}OD: \it #1}}
\def\ODdel#1{\bgroup\markoverwith{\textcolor{darkgreen}{\rule[0.5ex]{2pt}{1pt}}}\ULon{#1}}

% MV - Miroslav Vodolan
\definecolor{darkblue}{rgb}{0.0, 0.20, 0.85}
\def\MV#1{{\color{darkblue}MV: \it #1}}
\def\MVdel#1{\bgroup\markoverwith{\textcolor{darkblue}{\rule[0.5ex]{2pt}{1pt}}}\ULon{#1}}


\aclfinalcopy% Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here


\newcommand\BibTeX{B{\sc ib}\TeX}

\def\area#1{{\color{darkgreen}area:\it #1}}
\def\food#1#2{{Dial. state #1: \color{blue}food:\it #2}}
\def\pricerange#1{{\color{orange}pricerange:\it #1}}
\def\sys#1{{\color{purple}System: \it #1}}
\def\usr#1{{\color{brown}User: \it #1}}
\def\api#1{{\color{green}DB: \it #1}}

\title{Extracting Knowledge from Dialogue}

\author{Ondřej Plátek \\
  Charles University in Prague, Faculty of Mathematics and Physics \\
  Institute of Formal and Applied Linguistics \\
  Malostranské náměstí 25, 11800 Praha 1, Czech Republic\\
  {\tt oplatek@ufal.mff.cuni.cz}\\}

\date{}



\begin{document}
\maketitle
\begin{abstract}
Building a conversation agent is a demanding process which is typically simplified by a~using narrow and fixed conversation domain.
The most effective approaches either use a~very weak feedback and improve a~deployed dialogue system via reinforcement learning or use supervised learning and labeled data.
This work build on the success of the~methods above and focuses on designing conversational agents which will be able to:
\begin{itemize}
    \item collect explicit annotation interactively during the dialogue,
    \item enhance the knowledge base of a system by new facts,
    \item learn to recognize explicit reward signals in conversations.
\end{itemize}
Consequently, such conversational systems should:
\begin{itemize}
    \item need smaller amount of data and annotation needed for their optimization,
    \item self-improve based on the collected feedback.
\end{itemize}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The research of dialogue systems describes theories on how interlocutors communicate, communication techniques for humans and artificial systems are evaluated, and last but not how experimental artificial conversational systems are built.
Arguably, the most understood and commercial successful artificial systems are conversation agents playing the role of an expert in task oriented dialogue in a narrow domain.
Several research groups deployed such speech-to-speech dialogue systems on different domains, for example:
\begin{itemize}
    \item Let's go system~\cite{raux_lets_2005} helped the participants of experiments book a flight ticket and it was also deployed in Pittsburgh during night hours.
    \item The Cambridge group repeatedly uses Cambridge restaurant domain to evaluate experiments on crowd-sourced users where users search for a restaurant in Cambridge.
    \item The work of~\cite{dusek_sequence2sequence_2016} and \cite{vejman_martin_development_2015} evaluates their system on public transformation domain in Prague and New York, respectively, with crowd-sourced and also real users. 
\end{itemize}
All the mentioned works conclude that action selection (the task of dialogue management) plays a~central role in leading a dialogue.
However, the obvious differences between domains and the absence of widely accepted evaluation metrics for action selection do not allow a comparison of the deployed techniques and algorithms.

The lack of comparable research was the reason for organizing the~dialogue state tracking challenge (DSTC)~\cite{dstcwilliams}, which resulted in successful evaluation of many dialogue state trackers on the restaurant domain.
Dialogue state tracking represents the user's goal probabilistically within a predefined formalism such as dialogue acts~\cite{dstcwilliams,henderson2014second,hendersonthird}.
A dialogue act is a~triple $(type, slot's type, slot's value)$ where type is {\it inform, confirm,...}, slot type is {\it food, area, ...} and values are for example {\it Chinese, west, ...}.
The dialog state tracker updates distribution over slots and their values as the conversation evolves.
The~DSTC evaluates the quality of the distribution by easy to understand and widely accepted measures such as accuracy and L2 measure.\footnote{DSTC2 and DSTC3 challenges recommend using accuracy and L2 measures. In addition, one is advised not to evaluate the dialogue state trackers on the first turns where the dialogue state does not change.}
The improvements of dialogue state trackers enable more informed and thus better action selection which is the ultimate goal of a dialogue system.

% Most of the dialogue state trackers submitted to DSTC1, DSTC2 and DSTC3 used supervised learning to estimate the probabilities of dialogue state for given history\footnote{With the notable exception of~\cite{zilka_incremental_2015} tracker which perform simple Bayesian update without any learning and still achieves competitive results.}
The dialogue state is commonly defined by a~manually designed domain ontology.
Thanks to the DSTC success, where a handcrafted ontology was provided, it may seem easy to design such an~ontology\footnote{The Cambridge restaurant ontology had been polished over several years of research and it is actually rather simple.}.
However, we regard the manual onthology design as an arbitrary, costly and also error prone process.
Recently, it was shown by~\cite{wen_networkbased_2016} that dialogue state annotations are the only annotations in addition to conversation transcriptions needed for training an~end-to-end system jointly, so it became even more important to specify high-quality DST labels by the domain ontology.
In our recent work~\cite{platek2016wochat}, we proposed alternative annotations of dialogue history, which we describe in~Section~\ref{sec:experiments}.

We draw another conclusion from the dialogue state challenge: Using n-best lists from automatic speech recognition (ASR) helps just a little if compared with 1-best hypotheses even if ASR with high word error rates is in use.
In addition, the recent advances in speech recognition reduced the WER even on far-field ASR drastically~\cite{peddinti_jhu_2015,zhang_highway_2016}.
As a result, we observed that the focus of the research moved to text-to-text systems which can be easily integrated to speech to speech systems using a~one-best ASR and a~text-to-speech (TTS) modules.

We see the following research goals in the field of dialogue systems as the most important to address in next five years:
\begin{enumerate}
    \item Reducing the number of data and annotation needed for deploying task-oriented dialogue systems.
    \item Exploiting feedback and learned knowledge from live interaction with users.
    \item Efficient exploiting knowledge gained from training a~single domain agent for extending its domain. 
\end{enumerate}

We propose a research direction which aims to tackle the first two problems, and if successful, it may also help to solve the~domain extension problem.

We review current state-of-the-art end-to-end dialogue systems in Section~\ref{sec:e2end} with respect to our goals.
In Section~\ref{sec:learn_feedback}, we summarize how feedback is used for optimizing statistical models and how it is represented and extracted. 
In Section~\ref{sec:experiments} we describe our so far results, and we propose a plan of our future research in~Section~\ref{sec:future}. 
Finally in Section~\ref{sec:discussion}, we review our approach, discuss potential challenges and compare it to current research.


\section{End-to-End Conversational Agents}
\label{sec:e2end}
Building end-to-end conversation agents is a~new appealing approach for building conversation agents.
It reduces the task of building a~text-to-text dialogue system to training a single statistical model and thus optimizes the response generation process jointly with language understanding and state tracking and avoids accumulating errors along the pipeline.

Neural networks dominate the first attempts~\cite{williams2016end,bordes_learning_2016,weston2015endtoend_prereq} to build end-to-end conversational systems.
Using neural networks is an obvious choice because various neural network models achieve state-of-the-art results for optimizing traditional components of a dialogue system:
\begin{itemize}
    \item language understanding (LU) \cite{mairesse_spoken_2009} 
    \item dialogue state tracking (DST) \cite{williams_web-style_2014,henderson2014word,vodolan_hybrid_2015,platek_recurrent_2016}
    \item natural language generation (NLG) \cite{dusek_sequence2sequence_2016,wen_networkbased_2016}
    \item feedback/reward prediction \cite{su_learning_2015}
\end{itemize}
Neural networks models not only achieve the best results in all the mentioned tasks but the models are relatively straightforward to combine and optimize jointly.
The key to successfully training a neural network is to provide enough labeled data since the neural network typically learns a lot of unspecified structure and patterns.
Luckily, neural networks capture structures of data easily so less features need to be handcrafted and more importantly a sequence of components can be replaced by a single statistical model.
As consequence, the data for intermediate representation between the components is not needed for training the pipeline, but typically a~larger amount of training examples is required.

\subsection{Data annotation and loss function}
\label{sub:data_annotation}

Neural networks are commonly trained using supervised learning by maximizing directly the log likelihood of a conditional distribution $ P(Y| X) $ where the parameters of the distribution are estimated from training samples $ (\hat{X}, \hat{Y}) $.
Sequential problems such as dialogue conversation are formulated as a~problem of generating the~next reply $y_t$ given the history $h_t$.
In the~case of a~dialogue system, the history is represented by a~sequence of the~previous system utterances and user responses $ y_1, x_1, y_2, \dot x_{t-1}, y_{t-1}, x_t $.

Similarly to Hidden Markov Models (HMMs)~\cite{huang_hidden_1990}, Recurrent Neural Networks (RNNs) deal with potentially unlimited history by introducing a latent state $s_t$.\footnote{Unlike HMMs, they do not track probabilities in the hidden states but only use sufficient statistics.}
A~simple use of  RNNs~\cite{gers_learning_2000} for classification resembles the structure of HMMs for maximum likelihood estimation, e.g.\ , in ASR~\cite{huang_hidden_1990}, but RNNs model the observation probabilities discriminatively (see Figure~\ref{fig:encoder}).
The probability of action $ y_t $ is computed based on parameters for updating previous state $s_{t-1}$ to new state $s_t$ based on observation $x_t$.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{encoder}
    \caption{RNN encoder for classification. The output variables are conditioned on the hidden state. 
    This model was used as a~baseline in~\cite{platek_recurrent_2016} and it classifies DST state slots based on the last hidden state $h_t$ independently.}
\label{fig:encoder}
\end{figure}

In the work of~\cite{wen_networkbased_2016}, a neural end-to-end network model is trained for predicting a~next system reply from a dialogue history.
The model is trained not only from dialogue conversation transcriptions and the domain DB, but it also uses dialogue state annotations.
The annotations are used because the model trains a pipeline of two neural networks and the annotations are used as the data representation of the first network output and the input of the second neural network.
Its dialogue history is expressed as $h_t = y_1, x_1, s_1, y_2, \dot x_{t-1}, y_{t-1}, s_{t-1}, x_t, s_t $ where $s_t$ is the~annotation of dialogue state slots and their values e.g.\ {\it (food=Chinese, price\_range=expensive, area=west)}. 
The work optimizes the model using supervised learning by maximizing the likelihood of the~next word in a reply using cross-entropy training.

Cross-entropy training is the most successful method of training neural networks; However, maximizing the likelihood of the next word in a~response should not be the ultimate goal which conversational agents should achieve.
First, supervised approaches are limited by the~quality of the golden data.
Second, the independence assumptions of generating the~next word from the hidden state are not true in dialogue.
Consequently, there is plenty of room for improvement in modeling dialogue by end-to-end models.

\subsubsection*{Maximum likelihood: the most popular approach}\label{sub:maximum_likelihood}
Training neural networks for dialogue from gold labels using cross entropy for supervised learning brings several challenges:
\begin{itemize}
    \item predictions of the~next decision using the sequence-to-sequence model~\cite{bahdanau_neural_2014,sutskever_sequence_2014} are conditionally independent given the sufficient statistics of the hidden state of the decoding RNN.
    \item Dialogue responses of the system are often ambiguous and multiple options are equally possible.
        On the other hand, cross-entropy loss function models uncertainty and ambiguity in data in the same way, allowing only one correct option.
        In Equation~\ref{eq:cross}, $p(x)$ is a~distribution approximated by the~distribution of samples over training data and updates of the neural networks need to be performed only as difference between the predicted likelihood $-\log(q(x))$ and a~log one-hot distribution\footnote{One-hot distribution is a~categorical distribution where the single option is marked as gold and is assigned probability of one.}:
        \begin{equation}\label{eq:cross}
            H(p, q) = \sum_{x}{p(x) * (- \log q(x))}     
        \end{equation}
    \item Annotations of training pairs are costly and much more data is needed when annotations e.g.\ of dialogue state are not provided.
\end{itemize}

\subsubsection*{Weak supervision for reinforcement learning}\label{sub:batch_rl}

In this section, we introduce reinforcement learning (RL) as a~tool of our choice for optimizing the evaluation/reward/loss function e.g.\ user satisfaction or mimicking the whole dialogue.
Reinforcement learning is a~general framework for updating statistical model parameters even when a feedback for system's action is delayed or noisy~\cite{williams2016end,bahdanau_actor-critic_2016,wierstra_recurrent_2010}.
In the field of dialogue systems, the RL was successfully used to optimize parameters of Gaussian processes for selecting among several dozens of actions~\cite{gasic2011line}.
On the other hand, using relatively noisy feedback from a~few thousands of live conversations with user feedback at the end of dialogues is not convenient for training neural networks because they need to update a~large number of parameters and such feedback is too weak.

Note that RL was successfully used with supervised cross-entropy pre-training and later optimizing for not differentiable loss function.~\cite{williams2016end}.
However, such approach still uses the same labeled data and annotations as used for supervised learning.
The advantage of this approach is that it is able to naturally capture via the reward functions multiple valid actions and optimize directly the evaluation function.
Furthermore, multiple weak reward functions can be combined to form much stronger feedback signal~\cite{abbeel_apprenticeship_2004}.

Reinforcement learning performance quality depends on the reward information which can be used for updating parameters of the model and also on the amount of data available.
Typically the reward is also used as a~scoring function for evaluating dialogues.
However, if using the scoring function as reward function for reinforcement one would like to automate the computation, but only partial and not satisfactory automatic scoring functions were suggested~\cite{liu_how_2016,lowe_evaluation_2016} for dialogue systems. 
Note also that some RL algorithms such as Sarsa are on-policy algorithms and need to be trained using live deployed systems.
We focus only on algorithms which can be used in off-policy settings~\cite{sutton_reinforcement_1998} and does not require deployed system for their training.

\subsubsection*{Learning to collect feedback}\label{sub:irl}
The use of reinforcement or supervised learning assumes that the loss function is provided before the training.
One of the key properties of dialogues systems is that their domains change over time~\cite{yu_evolvable_2016}, so every predefined loss function becomes obsolete after some time.
In addition, specifying a good loss is notoriously hard since no standard measures for dialogue are widely accepted.

Inverse reinforcement learning (IRL)\footnote{Also known as active reward learning~\cite{su2016active}.} is a task of learning the loss (or reward) function used in RL~\cite{abbeel_apprenticeship_2004}.
IRL could be used for learning how to interpret user feedback, but the IRL is ill-posed problem in general~\cite{choi_inverse_2011} and is guaranteed to work only for special cases e.g.~\cite{abbeel_apprenticeship_2004,choi_inverse_2011}.
% \todo{Understand Choi better}
In our future work (see~Section~\ref{sec:future}) we aim to learn feedback from an~interactive conversation, but we plan to use IRL and similar approaches as reward shaping~\cite{su2016active} only for comparison.
We describe alternative approaches of collecting explicit annotation in Section~\ref{sec:learn_feedback}.

Note, that collecting feedback explicitly for later use and IRL is not mutually exclusive.
A~promising framework which may overcome vagueness of current system is adversarial networks, especially work of~\cite{dumoulin_adversarially_2016}\footnote{Picture is taken from \url{http://ishmaelbelghazi.github.io/ALI/}.}, which remotely resembles inverse reinforcement learning.
The adversarially learned inference (ALI) model is a deep directed generative model which jointly learns a generation network and an inference network using an adversarial process. 
The core idea is that one trains a generative model for dialogue system operator together with discriminator which attempts to recognize whether a generated dialogue is from the trained system or data sample from human (see Figure~\ref{fig:gan}).

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{gan_simple}
    \caption{Adversarial Learned Inference core idea~\cite{dumoulin_adversarially_2016} is to train a discriminator and a generator as one network with two objectives. 
    Discriminator separates real samples and generated samples and generator produce such artificial samples so that the discriminator cannot distinguish them from the real one. The algorithm was demonstrated on CIFAR image data-set and needs to be adapted for generating dialogues~\cite{krizhevsky_cifar-10_2014}.}
\label{fig:gan}
\end{figure}

We are also interested in this approach because one should be able to see which positive examples contributed the most for the performance of the generator (the dialogue system) by exploring the error updates and performance of the discriminator.
Thus the discriminator could provide us with automatic annotations. 

\subsection{Architectures for neural end-to-end dialogue systems}
\label{sub:nn_architectures}
Neural architectures are used in two use-cases of conversational agents;
\begin{itemize}
    \item Chatbots --- Such systems are trained without any structured knowledge from plain conversations.
    \item Task oriented systems --- The systems learn to play a~role of an~expert and provide information from a~database to users.
        They are trained in respect to the specific content of their database.
\end{itemize}

The work of~\cite{serban_multiresolution_2016} is an example of neural network architecture for chatbots which is able to mimic human-to-human conversations however quite often fails at capturing its semantics.
This line of research improves upon language modeling using neural networks~\cite{mikolov_efficient_2013} and adapts modeling of next word prediction by more efficiently representing the discourse structures.
The training of such models requires large corpora such as~\cite{lowe_ubuntu_2015}. 

On the other hand, researches working with task-oriented dialogue system train the models to extract common knowledge and discourse structures in the dialogue with help of a~database containing domain related information.
A limited domain also implicates that one is able to use only a~limited amount of data for training the systems.
The true challenge is how to model access to the system calls to the database because the calls are not recorded in the data since only plain text transcriptions of the dialogue are easy to acquire.
One can only deduce that a part of the response is a result from some database call.

The work of~\cite{wen_networkbased_2016} solved the problem by using the annotation of dialogue state which determines database calls and corresponding natural language response convenient for the~dialogue state. 
The simplistic system~\cite{williams2016end} showed that it is possible to use API calls\footnote{The system performs also other actions than accessed database so the term API call is used instead of DB calls.} without dialogue state annotation, but the system required {\it action mask} heuristics which determined whether an action is possible for the~dialogue context.

\section{Collecting Feedback}
\label{sec:learn_feedback}
Extracting feedback from dialogues is possible in several ways as described earlier: through fixed loss function, through learned loss function or explicitly through annotations.
The feedback can be stored either explicitly as annotations or in parameters of the model.
We focus on collecting explicit feedback because it can be exploited by any model via supervised or reinforcement learning.
Nowadays, the interactive feedback from users is either ignored or stored only in parameters of statistical models e.g.\ neural networks.
The crucial problem is that information represented in statistical models such as neural networks is notoriously hard to reuse for new architectures~\cite{oquab_learning_2014}.
To our knowledge, we will be the first trying to extract the explicit feedback automatically.
We consider it important task for automating dialogue systems because non-trivial expert work is needed for designing the heuristics or laborious effort is put into collecting annotations.

\section{Experiments}\label{sec:experiments}
Our work has focused on building an end-to-end conversation agent which eliminates laborious handcrafting but needs a~large portion data for training. 

We first describe our published works, then we introduce work in progress and finally we propose future work in the next Section~\ref{sec:future}. 

\subsection{Published work}\label{sec:published}
Our work so far has focused on developing an end-to-end task-oriented conversation agent on restaurant domain which is easy to train and provides a~reasonable baseline.
First, we verified that we are able to train recurrent neural networks for dialogue state tracking and achieve near state-of-the-art easily~\cite{platek_recurrent_2016}. 
We frame the DST as sequence-to-sequence problem and used encoder-decoder (see Figure~\ref{fig:dst_seq2seq}). 

\begin{table*}[tb]
\begin{center}
\begin{tabular}{l@{\quad}rll}
\hline
Model & Dev set & Test set\\
[2pt] \hline\rule{0pt}{12pt}
    EncDec  &   0.867 & 0.730 \\
\hline
    \cite{vodolan_hybrid_2015} & - & 0.745 \\
    \cite{zilka_incremental_2015} & 0.69 & 0.72 \\
    \cite{henderson2013deep} & - & 0.737 \\
\hline
    DSTC2 stacking ensemble~\cite{henderson2014second} & - & 0.789 \\
\hline
\end{tabular}
\caption{The~Accuracy of our DST encoder-decoder compared to other implementation. The first group contains our systems which use ASR output as input, the second group lists systems using also ASR hypothesis as input. The third group shows the results for ensemble model using ASR output and also live language understanding annotations.}
\end{center}
\label{tab:dstc}
\end{table*}

\begin{figure}[htb]
    \centering
    \includegraphics[width=1.0\linewidth]{dst_seq2seq}
    \caption{DST using an encoder-decoder RNN model.}
\label{fig:dst_seq2seq}
\end{figure}

Second, a dataset for training end-to-end system~\cite{platek2016wochat} was collected which focus on collecting easy-to-obtained annotations.
The dataset contains both human-to-human conversations and corresponding calls to database.
The crowdsource workers, which play the role of the system, annotate their response additionally which row of database they have used for answering the user query (see~Figure~\ref{fig:apicall}).
We argue that DB calls are much more natural and easy to obtain annotations both from crowd-sourcing and from life-system. 

\subsection{Work in progress}\label{sec:wip}

We are currently working on an~end-to-end task-oriented system which can be optimized as single component.
In contrast with very recent papers, our approach will hopefully need less annotations and doesn't use dialogue state labels as in~\cite{wen_networkbased_2016}, but it will use easier to obtain DB calls records~\cite{platek2016wochat}.

A crucial problem of end-to-end statistical models for a dialogue system is that the dialogue agent performs latent actions which appear rather stochastically due to lack of training data, but the latent actions follow strict logic.
The challenge for end-to-end models is to learn such logic in purely data-driven approach.
An example of such (partially) latent actions are calls to database.\footnote{Any action of an agent can be transformed to a~database call because the dialogue system may delegate the task through the database to other services which execute the actions. Typical example is a~weather information service.}
A system quering a~database to obtain some information is a partially latent action because the system presents only~the results of the action to the~user.
However, the user without the access to system database is not able to distinguish if the system is providing a valid answer.
The system for example may choose credible reply based based on its language model (see Figure~\ref{fig:encdec_lies}) which is plainly false.
An example of completely latent action is if the~user ``{\it orders pizza}''.
The system should insert the order into the reservation system represented in DB.
However, the user can only hope that the system executed the correct action and has to continue in the conversation. 
In the ``{\it pizza order}`` example, he or she happily waits at least 30 minutes for pizza delivery and if the dialogue system misunderstood the user and did not order the pizza, the user won't provide any feedback from which the system is able to learn.
As a result, if one wants to train a dialogue system one needs to design objective function and data representation which force the latent action correspond to system replies, so the logic of the latent actions is maintained.
Unfortunately, the feedback that a DB call does not correspond to the system reply is often not available as demonstrated in our ``{\it pizza}'' example.

\begin{figure*}[htb]
    \centering
    \includegraphics[width=1.0\linewidth]{dusek_seq2seq}
    \caption{Dialogue act items tracking and generation. A policy ensures that each access to DB corresponds to NLG plan what to say~\cite{dusek_sequence2sequence_2016}.}
\label{fig:dai_gen}
\end{figure*}

Typically, the dialogue manager (DM) updates the dialogue state, presents the results of DB call and suggests an NLG plan - all represented in a~discrete format such as dialogue state items (DAIs) (see Figure~\ref{fig:dai_gen}).
The DM policy executes operations on KB and produces an~NLG plan  which is compatible with KB actions~\cite{dusek_sequence2sequence_2016,young2010hidden}.
An~alternative simplistic approach is presented in~\cite{wen_networkbased_2016} where a KB operation is executed or skipped based on the dialogue state also represented by DAIs but the NLG uses only the dialogue state and the result of KB operation to generate a~reply where the KB calls are reduced to simple select statement with single variable.
We propose to predict directly which KB operation to use from the~hidden state $h_t$ of neural network which uses distributed representation and later predict the system reply.
The advantage is that one does not need to handcraft additional layer for tracking the dialogue state.
In our approach, the predicted reply is conditioned on the selected KB call's result and the same state $h_t$ state and will be generated in word by word manner.
Such model needs to be trained so the replies match the KB calls, but it allows flexibility how to communicate about the KB calls and their results.
We conditioned the reply based on the KB call and not vice versa, because we want the system to describe which action is actually used. 

Experiments with our first model showed that we have too few data and our model is too complicated to learn anything meaningful on the~DSTC2 dataset.
In the experiment, we used the DSTC2 dataset and we tried to mimic the system replies given the dialogue history and the DB of restaurants for the DSTC2 restaurant domain.
We design a heuristic function to create annotations in form of DB calls to simulate human annotation as described in~\cite{platek2016wochat}.
We used the DB calls as input features because they showed promising results in~\cite{platek_recurrent_2016} for DST.
Unfortunately, our model was not able to learn correlation between dialogue history inputs, content of the database and the systems replies.
It produces only sophisticated lies mimicking the systems replies and randomly choosing facts from database.
Consider example from Figure~\ref{fig:encdec_lies} where both plain encoder-decoder and our propose model decoded the same incorrect reply.
Our informal experiments showed that encoder-decoder is able to produce the exact template on DSTC2 dataset in over than 30 \% cases without checking the right named entities.
Additionally, we find out that over 73 \% templates are convenient but they differ only in incorrectly used named entities such telephone number as demonstrated in~Figure~\ref{fig:encdec_lies} on small manually checked subset of 100 replies from DSTC2 test set.
\begin{figure}[!ht]
    {\bf input:} {\it anatolia serves turkish food in the moderate price range what is the phone number and address} \\
    {\bf decoded:} {\it The phone number of {\bf meghna} is {\bf 01223 727410} . EOS} \\
    {\bf target:} {\it The phone number of anatolia is {\bf 01223 362372} and it is on 30 Bridge Street City Centre . EOS } \\
    \caption{For encoder-decoder framework is easy to learn templates but hard to extract semantic information from the query and the system database. Note, that the decoded reply produced telephone number but of a~completely different restaurant (not even the one called {\bf meghna}).}
\label{fig:encdec_lies}
\end{figure}

We found out that our model learned to ignore the database part of the model and propagated all the information through the encoder-decoder part.
We blame mainly huge ambiguity in the data and not enough training examples in the DSTC2 dataset so the model cannot learn the abstraction for representing constraints provided in the history for selecting the right row. 
\begin{figure*}[!bt]
    \centering
    \includegraphics[width=1.0\linewidth]{encdecdb}
    \caption{Computational graph of neural network model which demonstrates that conditioning on the database part was completely ignored by the trained model. We hoped that the attention will combine the information from the KB graph and the last hidden state $h_T$, but it put weight only on the last hidden state $h_t$ for all words as depicted by the yellow colour representing the last hidden state $h_t$. The output of the KB computation was an weighted embedding of rows of the database similarly as represented in Figure~\ref{fig:e2end_entities}.}
\label{fig:e2end_fail}
\end{figure*}

We aim to solve the problem by simplifying the model and learn to predict each row of database directly, predicting for each row of the DB by a binary classifier if it will appear in the database call results. 
The advantage is that such architecture allows for multiple possible results which are typical for user queries and it can be easily trained directly using supervised learning if we prepare annotations using simple heuristics or later to be fine-tuned with reinforcement learning.
As a side effect, each dialogue history and corresponding system reply training example will be expanded using heuristic functions to many possible replies over possible DB call results.
We expect that by explicitly modeling the data ambiguity the trained classifier will generalize better and finally learn to propagate constraints from history over database to results which will be semantically more accurate.
We are especially interested in improving the semantic compatibility of the answer which we measure based on the dialogue state annotation from DSTC2 data or DB calls in the dataset~\cite{platek2016wochat} as recall and accuracy.
\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{e2end_dbclassifiers}
    \caption{We are currently working on a binary classifiers model which only focus on selecting the right named entities for the reply from database table. 
    Each binary classifier predicts if the restaurant properties stored at corresponding row should be presented in the reply (see the grey column representing the binary decision of the classifier). 
    The dialogue history is simply encoded into series of embeddings $x_1, x_2, x_T$ for each word.
    The database is represented as set of rows.
    Note that the input words and the database entities are represented as embeddings which are trained. 
    The colors in the table represents the same embeddings used on different places.
    The row embeddings are combined embeddings of properties values where all property values are stored in one column.
    We found out that selecting the right template is quite easy task in comparison to providing the right entities to the template.
    }
\label{fig:e2end_entities}
\end{figure*}


\section{Future work}\label{sec:future}

Most of the current research focus on reducing handcrafted components from the dialogue system architectures.
A successful approach is to built an end-to-end system using neural networks which we introduced in~Section~\ref{sec:e2end} and proposed improvements in Section~\ref{sec:wip}. 
However, the most popular approaches require a lot of annotated data.
We suggest that a system should collect annotations which are used for its retraining. 
We aim at reducing both the expert work and annotated data needed for launching a narrow domain task oriented dialogue system.
Next, we will not only try to optimize action selection process of a~dialogue system, but we will also attempt to learn new facts and thus perform more informed decisions.
At the same time, such agent improves its performance by live interactions with users and the annotations may be helpful for adapting to a new domain.

We suggest following experiments to explore the crucial problems which need to be solved before a conversational agent is able to extract information through interaction and later used them for self-improvement.
The experiments are designed as a proof of concept on a~narrow domain and scaling up to larger or multiple domains are left for future work or as obvious extensions. 

\subsubsection*{Easy first decoding for dialogue state tracking using reinforce algorithm}
We show in work~\cite{platek_recurrent_2016} that modeling DST as sequence-to-sequence is easy-to-deploy, captures correlation between dialogue state values and also does nut suffer much from data sparsity.
However, the order of the dialogue state labels is arbitrary chosen and may not be optimal for prediction dialogue state.
In this experiment, we rephrase the DST task as a~sequence-to-set model by employing loss function which prefers if three hypotheses labels match the set of gold labels $\{food\_type, area, price\_range\}_{hypotheses} = \{food\_type, area, price\_range\}_{gold}$.
Such loss function is in sharp contrast with combined cross-entropy loss maximizing probability true labels in order food, area and price range which is used in sequence-to-sequence approach.
The sequence-to-set loss function is not smooth and cannot be differentiated, but we will use reinforce algorithm~\cite{williams_simple_1992} to update the model parameters.
We plan to pre-trained the model with cross-entropy updates and later fine tuned the weights with a~reinforce training.
% We do not plan to use the reinforce algorithm for tis less stable if one try to use larger steps size e.g. by AdaDelta algorithm~\cite{adadelta} as used for cross-entropy training.
Given the same model and the same number of parameters we expect the model trained with the reinforce algorithm to perform better than a~cross-entropy training with early stopping.
We assume that the reinforce algorithm will benefit from directly optimizing the evaluation function.
In addition, it will be interesting to explore which permutation of slots is best to use for DSTC2 dataset and if the best permutation differs for each dialogue.
Using the reinforce algorithm on well pre-trained model we want to examine if easy-first decoding of slots can perform better than arbitrary chosen order of predictions. 

\subsubsection*{Discovering the database queries}
The dialogue system generates two kinds of actions; responses and calls to its database.
The calls to the database influence the system replies in future turns by presenting their results or by changing results of next database calls.
This experiment focuses on discovering the database calls which should be used for querying the database from human-human conversations.

Even if the DB results are immediately presented the database operations are not obvious.  
The first example in Figure~\ref{fig:apicall} demonstrates that the system presents only one of possible results of the DB call to the user.
In the second example, the system replies the single possible answer {\it in the west part} to the query {\it select restaurant.area where name="India house"}.
However, the same result can be obtained by many other queries where the~name is replaced by other restaurants; $x=\{travellers\ rest, la\ margherita, \ldots\}$ all of them from west part of the city.
As a result, discovering only the queries which makes sense from human perspective is challenging 

We formulate the task as a classification problem where the classifier assign high probability to DB calls which are able to produce results compatible with the system reply and ideally the highest probability should be given the DB call intended by the user.

We propose to use the automatic and objective evaluation criteria. 
Namely, we will check if:
\begin{itemize}
    \item the system replies with valid property,
    \item the arguments of the DB call are present in the dialogue history.
    \item the gold answer is in the result of the DB call.
\end{itemize}
We will evaluate our classifier on the dataset collected in work~\cite{platek2016wochat} where if the entity is valid given the history the DB call is unambiguous. 
In addition, we would like to compare it human judgement on portion of the data.
\begin{figure}[!ht]
    \dots \\
    \usr{I would like a Chinese restaurant} \\
    \sys{In which area?} \\
    \usr{In the city center} \\
    \api{select restaurant.name where area="city center" and food="Chinese"}
    \sys{A golden house is a Chinese restaurant in the city center} 

    \dots \\
    \usr{Where is the India house restaurant located?}
    \api{select restaurant.area where name="India house"}
    \sys{It is located in the west part of the city.}
    \caption{Dialogue example with latent calls to system's DB.}
\label{fig:apicall}
\end{figure}

\subsubsection*{End-to-end system for consistent DB calls and system replies}
In this experiment, we will evaluate the extension of end-to-end system from Section~\ref{sec:wip} which should provide us with reasonable accuracy of selecting the right database call results.
We plan to frame the NLG part of the system as classification task of predefined templates for its simplicity.
The overall system (see Figure~\ref{fig:wochat}) will predict at each turn the DB call and a~template.
The template will have placeholders for the DB call arguments and most importantly for the DB call result.\footnote{Implicit confirmation by repeating the arguments is a~common strategy for grounding~\cite{meena_crowdsourcing_2014}.}

The evaluation will remain the same for choosing the right DB call and its result, the NLG part will be evaluated using accuracy on the golden data from~\cite{platek2016wochat} dataset and also using consistency with DB call and convenience for the dialogue history.
Accuracy evaluated with respect to the golden data is a rather strict criterion, because it may penalize valid responses unseen in the dataset, so we also try to account for other possibilities.
Consistency with DB call results, i.e.\ checking that the arguments of the templates are compatible with golden/predicted DB call, is only necessary requirement for valid template to hold, so it is only a weak measure.
Consequently, we plan to validate if a template is convenient reply for given history by crowdsourcing.

\subsubsection*{Misunderstandings data collection}
We assume that error handling is relatively frequent, one should be able to detect it and recover from it~\cite{skantze_error_2007}.
% \todo{mention early error detection}
In contrast, to~\cite{skantze_error_2007} who focused on ASR errors we focus on following sources of errors which lead to misunderstanding:
\begin{itemize}
    \item out-of-domain or out-of-application user query and inappropriate system reply,
    \item ambiguity in the context where one interpretation is intended by the user and the system choose the other one,
    \item poor action selection of reply or DB call for given history context.
\end{itemize}
First, we propose to use a special reply for out-of-domain user queries which explains users that his query is out-of-domain and presents system's domain and possibilities.~\cite{platek_self_2015}
With such strategy for handling out-of-domain queries and the errors when system does not inform about its skills, we see the action selection of another reply as an error for given dialogue history.
If the misunderstanding is not cause by out-of-domain query the system response may be plainly false for all cases or the user is aware that the system reply might be valid answer, but wants another one.

In this experiment we want to first evaluate how good are we able to detect misunderstanding and second how well are we able to recover from it in the live dialogue. 
We especially want to focus on the early misunderstanding detection.
We will use the dataset prepared in work~\cite{platek2016wochat} for dialogues without any misunderstanding.
Later, we will artificially introduce incorrect system replies to certain dialogue histories and we will collect new continuation of the dialogues after the intentionally introduced nonsensical system reply. 
We will also ask users to select the most convenient part of the dialogue where to place an out-of-domain question which they are interested about.
Again we will collect new follow up conversations and see how humans handle out-of-domain questions about which they do not know any information.
We will evaluate accuracy of classifying turns (pair of system and user utterance) into three categories:
\begin{itemize}
    \item user and system replies are both in the system domain 
    \item user uses out-of-domain utterance 
    \item user attempts to recover from misunderstanding
\end{itemize}
Optionally, we will investigate if the system uses correct response for recovering from misunderstanding.

We plan to run the experiment as pure Wizard of Oz experiment, and also using a~live deployed system.

\begin{figure*}[!tb]
    \centering
    \includegraphics[width=1.0\linewidth]{gui-annotators-system}
    \caption{End-to-end system data collection interface}
\label{fig:wochat}
\end{figure*}

\subsubsection*{Data augmentation through exploration}
The last of the planned experiments investigates whether our implementation of error recovery detection and our clarification strategies are robust enough to label and thus discover new valid actions for given dialogue history.
The idea key idea is that the dialogues tend to be repetitive in narrow domains of task oriented systems.
The repetitiveness may not seem natural for human users, and also if one want to train end-to-end systems repetitive data makes the system not only also repetitive too but also less robust.\footnote{The system replies are not only used as targets but also as input features for the system next reply} 
We want to mitigate the repetitiveness of the data by augmenting dialogues by paraphrases.

We will use the original repetitive conversations where the system has reasonable confidence of its action to intentionally slightly change the system reply.
If we do not detect misunderstanding in several cases of such new reply for given context, we will suppose we can include it as new data point. 
The new conversations will be added to original dataset.
We suppose that we want to improve an~end-to-end systems trained from conversational data such as described at Section~\ref{sec:e2end}.
The system is able to produce alternatives for each predicted action and also their confidence scores.
Using the scores we may not choose the recommended action by the system, but our active learning algorithm will decide on the alternative.
We will use coverage and precision of problematic situation described in~\cite{meena_datadriven_2016}.
We plan to use ALI generation and discrimination algorithm described in~Section~\ref{sub:irl} which is suitable for over-generating several candidates.
We hope that discriminator will learn to distinguish the responses also on semantic level of the system's domain which current approaches mostly ignore. 

\subsubsection*{Data discovery through misunderstanding}
Presenting incorrect information from DB to the users is a worth special attention.
Every database is in principle incomplete, and very quickly depreciates if the database contains information like bus stops, street names or telephone numbers which are all examples of entities used in most common task-oriented dialogue systems.
Currently, we want to focus only on the incorrect or outdated information and not the missing information.
We want to explore if a user is able to detect that system answer does not match reality and also if it is able to provide the correct answer.
In this experiment, we will use crowd-source workers to correct some intentionally outdated information about restaurant domain in Cambridge.
The interface displays just the dialogue history for the user and also the DB to the operator (see see~Figure~\ref{fig:wochat}).
For this experiment, a user will not be only informed about restaurant using text system reply which may be outdated, but a leaflet with updated address, price range and menu will be presented to the user.
The user will be instructed to tell the system that it is providing incorrect information.
We will investigate how often the user notice the mismatch between the information provided and the true state presented in the leaflet.
Second, we will evaluate how accurately our system is able to detect that user is correcting its information and how the system is able to parse the information from users answers.


\section{Discussion}
\label{sec:discussion}

We see the emerging end-to-end systems~\cite{williams2016end,weston2015endtoend_prereq,wen_networkbased_2016} as a~big step forward to reducing expert effort needed in building dialogue systems.
However, the human effort put into building even narrow domain task oriented system just shifted from precious expert work to more scalable crowd-source annotators effort~\cite{wen_networkbased_2016,serban_building_2015}.
We present series of experiments which should study how such scalable approach is pushed even further by collecting annotation through interactions.
In addition, the same methods can be used to extracting and updating knowledge from conversations which is almost unexplored direction how to extend dialogue system domain and knowledge.

The inverse reinforcement learning (IRL) is well established research direction of reducing the need for labeled data~\cite{abbeel_apprenticeship_2004}.
The work of~\cite{nouri_cultural_2012} uses IRL successfully for a toy task in cultural decision making in negotiations.
More recently,~\cite{su2016active} described active reward learning using unsupervised neural network embeddings and Gaussian processes and managed to greatly reduce the amount of data to deploy a system.
The problem with active reward learning of particular architecture is that when the single neural network architecture deprecates the new system can use all the knowledge stored in the system parameters in a very limited way.
Effectively, it means that only the same starting labeled data together with collected unlabeled data can be used for training new architectures.
We see that collecting annotation have bigger benefit from longer point of view and additionally it is completely orthogonal to active reward learning.

The topics of error detection and error recovery of dialogue systems has been described from several points of view, but we learned that most of the experiments are focused on ASR errors~\cite{skantze_error_2007}.
The work of~\cite{meena_datadriven_2016} analyzed how to detect and recover also from SLU and DM errors.
% \todo{Read and include work of~\cite{lopesspedial}.}
However, the work of~\cite{meena_datadriven_2016} primarily focused to discover errors offline and recommend designers which part of dialogues system needs more attention.

The work of~\cite{pappu_knowledge_2014} detects errors over multiple components and then employs post processing step which optimizes the components' pipeline jointly.
This attitude is no longer necessary because all our components are optimized jointly.
However, the work shows interesting insights on what might be the most common errors and it also uses an~error discovery for knowledge acquisition.
The authors report promising results especially on acquiring missing named entities and enriching the system knowledge base.

In our work, we want to follow up on their results, integrate our strategies to fully trainable system. 
The most importantly, we want collect annotations for improving the system itself in addition to learning new facts directly.
To our knowledge no other work described a dialogue system which stores user reward signal in explicit form, aka annotations, for its later optimization.
We would like to investigate its usefulness and compare it with current approaches such as active reward learning~\cite{su2016active} and zero-shot learning~\cite{vinyals_matching_2016}.\footnote{The zero-shot learning applies encoded prior information to extremely efficiently use few data samples provided in the user feedback.}


\section{Conclusion}
\label{sec:conclusion}
We presented our work in progress, motivated our past and suggested possible future experiments and discussed their difficulties.
Our experiments are mainly described as classification tasks where neural network statistical models will be used.
We choose neural networks because they not only efficiently exploit labeled data but also also can be fine-tuned with reinforcement training.
Our goal is to develop strategies which will help a conversational agent to collect annotations and facts useful to any statistical inference algorithm.

We realize that we have proposed several research directions, which will be difficult to explore in depth despite their similarities.
We plan to continue by exploring the directions in the order they has been presented, and we hope that working on the first problems will teach us what experiment should we explore next.

\section*{Acknowledgments}
This research was partly funded by the Ministry of Education, Youth and Sports of the Czech Republic under the grant agreement LK11221, core research funding, grant GAUK 1915/2015, and also partially supported by SVV project number 260 333. 
We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40c GPU used for this research.
Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085, provided under the program ``Projects of Large Research, Development, and Innovations Infrastructures''.

\bibliographystyle{acl2016}
\bibliography{literature}

\end{document}
